---
title: "Lab One, Part One"
author: "Kevin Lustig, Rebecca Nissan, Anuradha Passan, Giorgio Soggiu"
date: "3/1/2022"
output:
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document:
    toc: yes
    number_sections: yes
    toc_depth: 3
---
```{r packages install, include=FALSE}

library(dplyr)
library(stringr)
library(tidyverse)
library(scales)
library(readxl)
library(rstatix)
library(ggpubr)
library(tinytex)
```
\newpage
#  Foundational Exercises

## Professional Magic

### Type I Error of the test
The type I error rate (i.e. false positive) is the probability of rejecting the null hypothesis when it is correct. In this case, the type I error would be the probability of getting 0 or 6 for your test statistic (and therefore rejecting the null) given that the null is true, i.e. p = 1/2. 

$$ \text{Let }Z = X_1 + Y_1 + X_2 + Y_2 + X_3 + Y_3 $$
$$ P(Z = 0 \text{ or } Z = 6 | p= \frac{1}{2}) =$$
$$ P(Z = 0 | p= \frac{1}{2}) + P(Z = 6 | p= \frac{1}{2}) =$$
$$ \text{ Each flip of the pair is independent from all other flips of that pair.} $$
$$ [P(X_1 = 0 \text{ and } Y_1 = 0 | p= \frac{1}{2}) * P(X_2 = 0 \text{ and } Y_2 = 0 | p= \frac{1}{2}) * P(X_3 = 0 \text{ and } Y_3 = 0 | p= \frac{1}{2})] + $$ 
$$[P(X_1 = 1 \text{ and } Y_1 = 1 | p= \frac{1}{2}) * P(X_2 = 1 \text{ and } Y_2 = 1 | p= \frac{1}{2}) * P(X_3 = 1 \text{ and } Y_3 = 1 | p= \frac{1}{2})] =  $$
$$	[\frac{\frac{1}{2}}{2} * \frac{\frac{1}{2}}{2} * \frac{\frac{1}{2}}{2}] + [\frac{\frac{1}{2}}{2} * \frac{\frac{1}{2}}{2} * \frac{\frac{1}{2}}{2}]  = $$
$$ [\frac{1}{4} * \frac{1}{4} * \frac{1}{4}] + [\frac{1}{4} * \frac{1}{4} * \frac{1}{4}] = $$
$$ \frac{1}{32} $$

### Power of test given p = 0.75
The power of the test is equal to the probability of correctly rejecting the null hypothesis when the null is false and the alternative is true. In this case, the power would be the probability of getting 0 or 6 for our test statistic (and therefore rejecting the null) given that the alternative is true, i.e. p = 3/4.

$$ \text{Let }Z = X_1 + Y_1 + X_2 + Y_2 + X_3 + Y_3 $$
$$ P(Z = 0 \text{ or }  Z = 6 | p= \frac{3}{4}) =$$
$$ P(Z = 0 | p= \frac{3}{4}) + P(Z = 6 | p= \frac{3}{4}) =  $$
$$ \text{ Each flip of the pair is independent from all other flips of that pair.} $$
$$ [P(X_1 = 0 \text{ and } Y_1 = 0 | p= \frac{3}{4}) * P(X_2 = 0 \text{ and } Y_2 = 0 | p= \frac{3}{4}) * P(X_3 = 0 \text{ and } Y_3 = 0 | p= \frac{3}{4})] + $$
$$[P(X_1 = 1 \text{ and } Y_1 = 1 | p= \frac{3}{4}) * P(X_2 = 1 \text{ and } Y_2 = 1 | p= \frac{3}{4}) * P(X_3 = 1 \text{ and } Y_3 = 1 | p= \frac{3}{4})] =  $$
$$	[\frac{\frac{3}{4}}{2} * \frac{\frac{3}{4}}{2} * \frac{\frac{3}{4}}{2}] + [\frac{\frac{3}{4}}{2} * \frac{\frac{3}{4}}{2} * \frac{\frac{3}{4}}{2}] = $$
$$ [\frac{3}{8} * \frac{3}{8} * \frac{3}{8}] + [\frac{3}{8} * \frac{3}{8} * \frac{3}{8}] = $$
$$ \frac{27}{512} + \frac{27}{512} = $$
$$ \frac{27}{256} $$

\newpage
## Wrong Test, Right Data 
In the Likert scale, the meaningful distance between the different scale points is not consistent. That is, assuming the Likert scale for the websites survey includes five points from 1 = "Very Unsatisfied" to 5 = "Very Satisfied," with 3 being "Neutral," we cannot say that a change from 1 to 3 and from 2 to 4 are equivalent quantifiable changes in opinion ^[At least, not with only five scale points; see, e.g.: Huiping Wu and Shing-On Leung, “Can Likert Scales Be Treated as Interval Scales?—a Simulation Study,” Journal of Social Service Research 43, no. 4 (June 2017): pp. 527-532, https://doi.org/10.1080/01488376.2017.1329775.]. In fact, the change in quality of experience necessary for a given respondent to go from Very Unsatisfied with one site to Neutral with the other may be considerably less than the change needed to go from Unsatisfied to Satisfied, though these each consist of a difference of two points. Therefore, though the values produced are numeric, these data violate one of the assumptions for a paired t-test -- the use of metric, rather than ordinal, data. 

A paired t-test relies on metric data because, like other related tests including the z-test, it is fundamentally a calculation of the difference of means between reference groups. A paired t-test would ask of our survey data: is the mean difference between paired opinion scores different than what we would expect if there were no preference for either website (mean difference within pairs = 0)? Stated otherwise, on average across all respondents, how likely is it that there is really a preference for one site or the other, and how large a preference? However, because of the aforementioned limitation of Likert scale values, we cannot meaningfully parse a mean paired disparity of e.g., +2, because to calculate this requires assuming that non-comparable changes from any one Likert scale point to another are equivalent. The mean of the paired differences is thus meaningless. It is even difficult to trust the directionality of the mean difference across all pairs (respondents like the mobile website more or less than the regular website without regard to how much), as in calculating a mean value purely from the raw Likert scale scores, we may calculate an incorrect value by not correctly taking into account the "weights" of the differences of opinion in, again, Very Unsatsified and Neutral versus Unsatisfied and Satisfied. It's possible to conceive of a scenario in which even the sign of the mean difference is therefore incorrect. 

It is this last point on directionality that suggests an alternative approach to this analysis. A non-parametric paired sign test allows us to analyze our ordinal data provided the observations are independent and identically distributed. It does not attempt, like the t-test, to quantify the size of the difference in opinion within pairs, if any. Rather, it treats all positive changes in opinion as equivalent, and does likewise with all negative changes. This alternative test has two main drawbacks. First, it does not have the statistical power of a paired t-test. Second, it loses substantial information present in the original survey responses in the form of the exact values within each paired set of responses. However, in doing so, it allows us to avoid the inaccurate mean calculation of the t-test, and focus on a more accurate analysis of a simpler question: do respondents prefer one website over the other? In looking solely at increases or decreases in opinion score, the paired sign test therefore gives us a reasonable expectation of finding such an effect if one is present in the data. 

\newpage
## Test Assumptions

### World Happiness 
*Scenario:* We have two variables: Life.Ladder and Log.GDP.per.Capita, and we want to see whether people in countries with high GDP per capita are more or less happy than people in countries with low GDP per capita.

*Proposed test:* Two Sample t-Test

*Test Assumptions:* \newline
1. Metric variables\newline
2. Random variables are independent and identically distributed (hereby referred to as i.i.d.)\newline
3. Normality of random variables\newline

The "Life Ladder" score (LLS) variable is composed of continuous values. Data can be classified and the distances between values make sense. This point is important considering that the Two-Sample t-Test aims at comparing means of two random variables.
The "Life Ladder" seems to correspond to a metric variable which satisfies the first assumption. The "Log GDP per capita" is used to divide the studied population into two groups and is not directly used by the test.

Assuming that data collection and transformation protocols were followed in similar fashion throughout the data collection and transformation activities, it can be assumed that the random variables are independent and identically distributed. 
```{r, include=FALSE}
# Load in dataset 
wh_data <- read.csv('datasets/happiness_WHR.csv')

# Select necessary variables 
wh_data <- wh_data %>% select(Life.Ladder, Log.GDP.per.capita)
```
Now moving on to investigating normality. A quick summary below of both "Life.Ladder" and "Log.GDP.per.capita" variables reveals that "Log.GDP.per.capita" contains 13 Na(s) values. These will be omitted for the rest of this investigation. Additionally, the summary below reveals the mean GDP per capita which will be used as the point to separate low and high GDP per capita countries. 

**Summary: Life Ladder Score and GDP per Capita**
```{r, echo=FALSE, comment=""}
# Get a quick look at the two relevant variables 
summary (wh_data)
```

Now that the data has been split between low and high GDP per capita countries, a quick look is taken at their respective distribution summaries for the LLS. 
```{r, echo=FALSE, comment=""}
# Calculate the mean GDP per capita (will be useful in sorting high and low GDP per capita countries)
## But first remove rows that have NAs 
wh_data <- na.omit(wh_data)
gdp_capita_mean <- round(mean(wh_data$Log.GDP.per.capita), digits = 2)
```
**Summary: Life Ladder Score for High GDP per Capita Countries**
```{r, echo=FALSE, comment=""}
## Now split into the high and low gdp per capita country groups
high_gdp_cap <- wh_data %>% filter(Log.GDP.per.capita > gdp_capita_mean)
summary(high_gdp_cap$Life.Ladder)
``` 
**Summary: Life Ladder Score for Low GDP per Capita Countries**
```{r, echo=FALSE, comment=""}
low_gdp_cap <- wh_data %>% filter(Log.GDP.per.capita < gdp_capita_mean)
summary(low_gdp_cap$Life.Ladder)
```
A quick glance gives the impression that the LLS for both low and high GDP per capita countries may be skewed a bit to the left. This can be confirmed by looking at a visualization of the distribution via histograms (done below). The x-axis displays the Life Ladder Scores and the y-axis the frequency of each Life Ladder Score values. 
```{r, echo=FALSE, comment="", fig.width=4, fig.height=2, fig.align='center'}
# Observe the histograms of both variables 
gghistogram(high_gdp_cap, main = 'LLS: High GDP per Capita Countries', x = 'Life.Ladder', fill= 'steelblue', bins = 10,  xlab = 'Life Ladder Scores', ylab = 'Count')
```

```{r, echo=FALSE, comment="", fig.width=4, fig.height=2, fig.align='center'}
#hist($) #fix #make pretty 
gghistogram(low_gdp_cap, main = 'LLS: Low GDP per Capita Countries', x = 'Life.Ladder', fill= 'steelblue', bins = 10,  xlab = 'Life Ladder Scores', ylab = 'Count')
```
The distribution for high GDP per capita countries is left skewed and contains two peaks. The overall shape does not seem to represent a normal distribution. The second distribution for low GDP per capita countries contains a single peak closer to the middle of the distribution and shows a symmetric behavior, indicating perhaps that there may be some normality. Considering the samples size, another approach can be taken:

The Central Limit Theorem (CLT) states that the distribution of a standardize sample mean converge to a normal curve distribution as the sample size tends to infinity. The CLT requires that data are IID and the population has finite variance. The first statement has already been verified. The finite variance one can be visually assessed via the samples distributions. 
As the Samples distributions are not heavy tailed distributions, we can assume (with some cautious) that each population has a positive variance. In that context, the CLT seems applicable.

**Conduct the test?**\newline
Given that the assumptions are satisfied including the normality assumption, the t-test is applicable.

### Legislators 

*Scenario:* Test whether Democratic or Republic senators are older, with two variables party and age (age needs to be calculated from DOB). 

*Proposed test:* Wilcoxon Rank Sum Test

*Test Assumptions:* \newline
1. Metric variable or ordinal variables \newline 
2. i.i.d. \newline
3. Same shape and spread of the two samples \newline

The Wilcoxon Rank Sum Test is less restrictive than the Two Sample t-Test. The metric assumption and IID are still required but not the normality of the data anymore.

The age variable (of the legislators) is made of continuous values in which the distances between the values make sense. The variable is metric and satisfies the first assumption. 

The ages of the legislators come from the same distribution (politicians' ages). Moreover, a politician's age does not provide information about the age of any other politician, which satisfies the independence condition and thus the IID, satisfying the second assumption. 

A summary of the two sample distributions alongside visual aids (histograms and box and whisker plot) will help determine whether tthe third assumption (same shape and spread) is satisfied. 
```{r, echo=FALSE, comment=""}
# Load in Data set 
leg_data <- read.csv('datasets/legislators-current.csv')

# Select necessary variables and calculate age 
leg_data  <- leg_data %>% 
  select(birthday, party, type) %>%
  filter(type == 'sen') %>%
  mutate(age = as.numeric(difftime(Sys.Date(), as.Date(birthday), unit = "weeks"))/52.25)
leg_data <- leg_data %>% select(party, age)
```

```{r, echo=FALSE, comment=""}
# Split the data by party
dem_sen <- leg_data %>% filter(party == 'Democrat')
rep_sen <- leg_data %>% filter(party == 'Republican')
indep <- leg_data %>% filter(party == 'Independent') 
## We see all Senators have been accounted for as the number of rows of above 3 data frames adds up to 100 
```
Summary: Democrat Senators
```{r, echo=FALSE, comment=""}
# Get a quick look at the relevant variables (Democrats and Republicans only) 
summary(dem_sen$age)
x <-var(dem_sen$age)
```

Summary: Republican Senators
```{r, echo=FALSE, comment=""}
# Get a quick look at the relevant variables (Democrats and Republicans only) 
summary(rep_sen$age)
```
Looking at the summaries of the two samples, the distributions do not seem to be too different, as evidenced by the close means, 1st quartile, and maximum. 

```{r, echo=FALSE, comment="", fig.width=5, fig.height=3, fig.align='center'}
# Map smoke to fill, make the bars NOT stacked, and make them semitransparent
leg_data1 <- leg_data %>% filter(party != 'Independent')
ggplot(leg_data1, aes(x = age, fill = party)) +
  geom_histogram(position = "identity", alpha = 0.4, bins = 15) + ggtitle("Age of Legislators") +
  xlab("Age") + ylab("Count")+labs(fill='Party')
```

```{r, echo=FALSE, comment="", fig.width=5, fig.height=4, fig.align='center'}
boxplot( rep_sen$age, dem_sen$age, data = dem_sen,
        main="Age of Legislators",
        xlab="Age",
        col = c('lightpink','lightblue'), 
        horizontal = TRUE, 
        names = c('Democrat', 'Republican'))
```
Looking closer at the sample distributions in the above figures, it still doesn't seem clear whether the spread and shape of the distributions are the same. To double check, the Ansari Bradley test can be done to test this. The null hypothesis in this test is that the two population distribution functions corresponding to the two samples are identical against the alternative hypothesis that they differ.^["Ansari-Bradley Test", https://www.quality-control-plan.com/StatGuide/ansari.htm]

```{r, echo=FALSE, comment="", fig.width=5, fig.height=4, fig.align='center'}
ansari.test(dem_sen$age, rep_sen$age)
```
The results from the Ansari-Bradley test shows a p-value of 0.2162, indicating that the null should not be rejected and that it is likely that the spread and shape of the distribution functions corresponding to the two samples are the same. The third assumption seems to be satisfied.

**Conduct the test?**\newline
Given that all the assumption seems to be satisfied, the Wilcoxon Rank Sum test would be applicable in this case. 

### Wine and Health 

*Scenario:* We want to test whether these countries have more deaths from heart disease or liver disease.

*Proposed test:* Wilxocon Signed Rank Test

*Test Assumptions:* \newline
1. Metric Variables \newline
2. i.i.d.\newline
3. Paired data \newline
4. Difference is symmetric\newline

The "heart" and "liver" variables are both composed of values obtained by counting, which make these discrete variables. Additionally, the values of both observations can be compared and ranked as they share the same zero-base reference and counting granularity. Both random variables are measured on a similar metric scale. Thus the first assumption is verified.

The heart disease and liver disease variables come from their respective distributions and with the assumption that the data was collected and transformed following the same protocol, the second assumption of independent and identically distributions can be satisfied. 

Another assumption of the Wilcoxon signed rank test is that it is conducted on paired data. In this case since the data is coming from the same test subject (the selected group of countries), it is confirmed that this assumption is satisfied. 
```{r, echo=FALSE, comment=""}
# Load in dataset 
library(wooldridge)
wine_data  <- wine
# Select necessary variables
wine_data <- wine_data %>% 
  select(country, heart, liver)
```

```{r, echo=FALSE, comment="", fig.width=5, fig.height=3, fig.align='center'}
# Test for symmetry
## First calculate the difference between Heart and Liver 
wine_data <- wine_data %>% mutate(difference = heart - liver)

boxplot( wine_data$difference, data = wine_data,
        main="Difference between Heart Disease & Liver Disease",
        xlab="Number of Deaths",
        col = 'lightblue', 
        horizontal = TRUE)
```
The distribution of the difference between the paired samples  "heart" and "liver" is a bit left skewed. Moreover this distribution does not seem to be symmetric around the median. However, one should not expect the differences in distribution to be perfectly symmetric especially when the sample size is small (n=21). Thus, the assumption of symmetry is difficult to assess here but could be considered as verified.

**Conduct the test?**\newline
The Wilcoxon Signed Rank Test can be applied in that context, with some caution. 

### Attitudes Toward the Religion 

*Scenario:* Investigate whether the U.S. population feels more positive towards Protestants or Catholics. 

*Proposed Test:* Paired t-Test

*Test Assumptions:* \newline
1. Metric Variables\newline
2. i.i.d.\newline
3. Paired data\newline
4. Normality\newline

The feeling thermometer variables towards both Protestants and Catholics is continuous and metric in nature since the respondents can choose their rating on a continuous scale from 0 to 100. Thus the first condition is satisfied 

The two feeling thermometer variables seem to also be independent and identically distributed assuming that the conductors of the 2004 General Social Survey were following the same protocol in collecting and transforming these data - satsifying the second condition. 

The paired t-test requires the data be "paired." This is true in this case because the test-subjects, i.e. the respondents, for both the feeling thermometer towards Protestants and Catholoics are the same - satisfying the third condition. 
```{r, echo=FALSE, comment=""}
# Load in dataset 
rel_data <- read.csv('datasets/GSS_religion.csv')

# Select necessary variables 
rel_data <- rel_data %>% select(cathtemp, prottemp)
```

```{r, echo=FALSE, comment=""}
# Get a quick look at the relevant variables 
summary(rel_data)
```

```{r, echo=FALSE, comment="", fig.width=4, fig.height=2, fig.align='center'}
#Observe the histograms for both variables 
gghistogram(rel_data$cathtemp, fill= 'steelblue', bins = 7, xlab = 'Feeling Thermometer Rating', ylab = 'Count', main = 'Distribution of U.S. Feelings, Catholics')
```

```{r, echo=FALSE, comment="", fig.width=4, fig.height=2, fig.align='center'}
gghistogram(rel_data$prottemp, fill= 'steelblue', bins = 7, xlab = 'Feeling Thermometer Rating', ylab = 'Count', main = 'Distribution of U.S. Feelings, Protestants')
```

```{r, echo=FALSE, comment="", fig.width=4, fig.height=2, fig.align='center'}
diff_prottemp_cathtemp <- as.numeric(rel_data$prottemp - rel_data$cathtemp)
gghistogram(diff_prottemp_cathtemp, fill= 'steelblue', bins = 7, xlab = 'Feeling Thermometer Rating', ylab = 'Count', main = 'Distribution of Feelings Difference')
```

Looking at the above summary and two histograms it does not look like the distributions are normal. The differences histogram looks symmetrical but not necessarily normal. The Shapiro-Wilk test  conducted below will confirm these observations. 

```{r, echo=FALSE, comment=""}
# Conduct a Shapiro test to also help determine normality 
shapiro.test(rel_data$cathtemp)
```
The results of the Shapiro-Wilk test on the feelings thermometer towards Catholics, has a p-value that is close to 0, indicating this variable does not have a normal distribution.  
```{r, echo=FALSE, comment=""}
# Conduct a Shapiro test to also help determine normality 
shapiro.test(rel_data$prottemp)
```
The results of the Shapiro-Wilk test on the feelings thermometer towards Protestants, has a p-value that is close to 0, indicating this variable does not have a normal distribution. 

```{r, echo=FALSE, comment=""}
# Conduct a Shapiro test to also help determine normality 
shapiro.test(diff_prottemp_cathtemp)
```
For the difference, the results also show here a p-value less than 0.05, indicating that the difference distribution is not normal. 

Based on the results of the visual investigation and Shapiro-Wilk tests, the third condition of normality is not met. 

**Conduct the test?** 
\newline
The distribution of the differences between US population feelings toward Catholics and Protestants is not normally distributed (considering both graph and Shapiro test values). The normality distribution of differences is one of the key criteria to apply a Paired t-test. Even though the other criteria regarding metric variables, independent and identical distributions, and paired data are met, the normality assumption is not verified. Hence the paired t-test can not be applied in this context.
