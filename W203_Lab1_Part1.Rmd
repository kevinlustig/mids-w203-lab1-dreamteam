---
title: "Lab One, Part One"
author: "Kevin Lustig, Rebecca Nissan, Anuradha Passan, Giorgio Soggiu"
date: "3/1/2022"
output:
  pdf_document:
    toc: yes
    number_sections: yes
    toc_depth: 3
  word_document:
    toc: yes
    toc_depth: '3'
---
```{r packages install, include=FALSE}

library(dplyr)
library(stringr)
library(tidyverse)
library(scales)
library(readxl)
library(rstatix)
library(ggpubr)
library(tinytex)
```
\newpage
#  Foundational Exercises

## Professional Magic

### Type I Error of the test
The type I error rate (i.e. false positive) is the probability of rejecting the null hypothesis when it is correct. In this case, the type I error would be the probability of getting 0 or 6 for your test statistic (and therefore rejecting the null) given that the null is true, i.e. p = 1/2. 

$$ \text{Let }Z = X_1 + Y_1 + X_2 + Y_2 + X_3 + Y_3 $$
$$ P(Z = 0 \text{ or } Z = 6 | p= \frac{1}{2}) =$$
$$ P(Z = 0 | p= \frac{1}{2}) + P(Z = 6 | p= \frac{1}{2}) =$$
$$ \text{ Each flip of the pair is independent from all other flips of that pair.} $$
$$ [P(X_1 = 0 \text{ and } Y_1 = 0 | p= \frac{1}{2}) * P(X_2 = 0 \text{ and } Y_2 = 0 | p= \frac{1}{2}) * P(X_3 = 0 \text{ and } Y_3 = 0 | p= \frac{1}{2})] + [P(X_1 = 1 \text{ and } Y_1 = 1 | p= \frac{1}{2}) * P(X_2 = 1 \text{ and } Y_2 = 1 | p= \frac{1}{2}) * P(X_3 = 1 \text{ and } Y_3 = 1 | p= \frac{1}{2})] =  $$
$$	[\frac{\frac{1}{2}}{2} * \frac{\frac{1}{2}}{2} * \frac{\frac{1}{2}}{2}] + [\frac{\frac{1}{2}}{2} * \frac{\frac{1}{2}}{2} * \frac{\frac{1}{2}}{2}]  = $$
$$ [\frac{1}{4} * \frac{1}{4} * \frac{1}{4}] + [\frac{1}{4} * \frac{1}{4} * \frac{1}{4}] = $$
$$ \frac{1}{32} $$

### Power of test given p = 0.75
The power of the test is equal to the probability of correctly rejecting the null hypothesis when the null is false and the alternative is true. In this case, the power would be the probability of getting 0 or 6 for our test statistic (and therefore rejecting the null) given that the alternative is true, i.e. p = 3/4.

$$ \text{Let }Z = X_1 + Y_1 + X_2 + Y_2 + X_3 + Y_3 $$
$$ P(Z = 0 \text{ or }  Z = 6 | p= \frac{3}{4}) =$$
$$ P(Z = 0 | p= \frac{3}{4}) + P(Z = 6 | p= \frac{3}{4}) =  $$
$$ \text{ Each flip of the pair is independent from all other flips of that pair.} $$
$$ [P(X_1 = 0 \text{ and } Y_1 = 0 | p= \frac{3}{4}) * P(X_2 = 0 \text{ and } Y_2 = 0 | p= \frac{3}{4}) * P(X_3 = 0 \text{ and } Y_3 = 0 | p= \frac{3}{4})] + [P(X_1 = 1 \text{ and } Y_1 = 1 | p= \frac{3}{4}) * P(X_2 = 1 \text{ and } Y_2 = 1 | p= \frac{3}{4}) * P(X_3 = 1 \text{ and } Y_3 = 1 | p= \frac{3}{4})] =  $$
$$	[\frac{\frac{3}{4}}{2} * \frac{\frac{3}{4}}{2} * \frac{\frac{3}{4}}{2}] + [\frac{\frac{3}{4}}{2} * \frac{\frac{3}{4}}{2} * \frac{\frac{3}{4}}{2}] = $$
$$ [\frac{3}{8} * \frac{3}{8} * \frac{3}{8}] + [\frac{3}{8} * \frac{3}{8} * \frac{3}{8}] = $$
$$ \frac{27}{512} + \frac{27}{512} = $$
$$ \frac{27}{256} $$

\newpage
## Wrong Test, Right Data 
In the Likert scale, the meaningful distance between the different scale points is not consistent. That is, assuming the Likert scale for the websites survey includes five points from 1 = "Very Unsatisfied" to 5 = "Very Satisfied," with 3 being "Neutral," we cannot say that a change from 1 to 3 and from 2 to 4 are equivalent quantifiable changes in opinion ^[At least, not with only five scale points; see, e.g.: Huiping Wu and Shing-On Leung, “Can Likert Scales Be Treated as Interval Scales?—a Simulation Study,” Journal of Social Service Research 43, no. 4 (June 2017): pp. 527-532, https://doi.org/10.1080/01488376.2017.1329775.]. In fact, the change in quality of experience necessary for a given respondent to go from Very Unsatisfied with one site to Neutral with the other may be considerably less than the change needed to go from Unsatisfied to Satisfied, though these each consist of a difference of two points. Therefore, though the values produced are numeric, these data violate one of the assumptions for a paired t-test -- the use of metric, rather than ordinal, data. 

A paired t-test relies on metric data because, like other related tests including the z-test, it is fundamentally a calculation of the difference of means between reference groups. A paired t-test would ask of our survey data: is the mean difference between paired opinion scores different than what we would expect if there were no preference for either website (mean difference within pairs = 0)? Stated otherwise, on average across all respondents, how likely is it that there is really a preference for one site or the other, and how large a preference? However, because of the aforementioned limitation of Likert scale values, we cannot meaningfully parse a mean paired disparity of e.g., +2, because to calculate this requires assuming that non-comparable changes from any one Likert scale point to another are equivalent. The mean of the paired differences is thus meaningless. It is even difficult to trust the directionality of the mean difference across all pairs (respondents like the mobile website more or less than the regular website without regard to how much), as in calculating a mean value purely from the raw Likert scale scores, we may calculate an incorrect value by not correctly taking into account the "weights" of the differences of opinion in, again, Very Unsatsified and Neutral versus Unsatisfied and Satisfied. It's possible to conceive of a scenario in which even the sign of the mean difference is therefore incorrect. 

It is this last point on directionality that suggests an alternative approach to this analysis. A non-parametric paired sign test allows us to analyze our ordinal data provided the observations are independent and identically distributed. It does not attempt, like the t-test, to quantify the size of the difference in opinion within pairs, if any. Rather, it treats all positive changes in opinion as equivalent, and does likewise with all negative changes. This alternative test has two main drawbacks. First, it does not have the statistical power of a paired t-test. Second, it loses substantial information present in the original survey responses in the form of the exact values within each paired set of responses. However, in doing so, it allows us to avoid the inaccurate mean calculation of the t-test, and focus on a more accurate analysis of a simpler question: do respondents prefer one website over the other? In looking solely at increases or decreases in opinion score, the paired sign test therefore gives us a reasonable expectation of finding such an effect if one is present in the data. 

\newpage
## Test Assumptions

### World Happiness 
*Scenario:* We have two variables: Life.Ladder and Log.GDP.per.Capita, and we want to see whether people in countries with high GDP per capita are more or less happy than people in countries with low GDP per capita.

*Proposed test:* Two Sample t-Test

*Test Assumptions:* \newline
1. Metric variables\newline
2. Random variables are independent and identically distributed (hereby referred to as i.i.d.)\newline
3. Normality of random variables\newline

The "Life Ladder" score (LLS) variable is composed of continuous values. Data can be classified and the distances between values make sense. This point is important considering that the Two-Sample t-Test aims at comparing means of two random variables.s
The "Life Ladder" seems to correspond to a metric variable which satisfies the first assumption. The "Log GDP per capita" is used to divide the studied population into two groups and is not directly used by the test. (Thus, the data type considerations related to the "Log GDP per capita" are not useful.) 

Validating the independence of the random samples seems hazardous assuming that countries and people might be linked somehow. In other words, knowing some information about one country might provide information/insights about neighboring countries. The second assumption is not completely verified.

```{r, include=FALSE}
# Load in dataset 
wh_data <- read.csv('datasets/happiness_WHR.csv')

# Select necessary variables 
wh_data <- wh_data %>% select(Life.Ladder, Log.GDP.per.capita)
```
A data exploration of both "Life.Ladder" and "Log.GDP.per.capita" variables has revealed that "Log.GDP.per.capita" contains 13 Na(s) values. 
Countries having Na values for the GDP will be omitted for the rest of the study. Two histograms are plotted to help visualizing the distribution of the data.
The x-axis displays the Life Ladder Scores and the y-axis the frequency of each Life Ladder Score values. These distributions are presented for countries from the "high GDP per capita"  and "Low GDP per capita" groups.

Summary: Life Ladder Score and GDP per Capita
```{r, echo=FALSE, comment=""}
# Get a quick look at the two relevant variables 
summary (wh_data)
```

```{r, echo=FALSE, comment=""}
# Calculate the mean GDP per capita (will be useful in sorting high and low GDP per capita countries)
## But first remove rows that have NAs 
wh_data <- na.omit(wh_data)
gdp_capita_mean <- round(mean(wh_data$Log.GDP.per.capita), digits = 2)
```
Summary: Life Ladder Score and GDP per Capita > Sample Mean
```{r, echo=FALSE, comment=""}
## Now split into the high and low gdp per capita country groups
high_gdp_cap <- wh_data %>% filter(Log.GDP.per.capita > gdp_capita_mean)
summary(high_gdp_cap)
``` 
Summary: Life Ladder Score and GDP per Capita < Sample Mean
```{r, echo=FALSE, comment=""}
low_gdp_cap <- wh_data %>% filter(Log.GDP.per.capita < gdp_capita_mean)
summary(low_gdp_cap)
```

```{r, echo=FALSE, comment="", fig.width=4, fig.height=2, fig.align='center'}
# Observe the histograms of both variables 
gghistogram(high_gdp_cap, main = 'LLS: High GDP per Capita Countries', x = 'Life.Ladder', fill= 'steelblue', bins = 10,  xlab = 'Life Ladder Scores', ylab = 'Count')
```

The distribution is left skewed and contains two peaks. The overall shape does not represent a normal distribution.
The second distribution (below) contains one single peak closer to the middle of the distribution and shows a symmetric behavior. In that sense, the second distribution seems to be a better approximation of the normal distribution.
 

```{r, echo=FALSE, comment="", fig.width=4, fig.height=2, fig.align='center'}
#hist($) #fix #make pretty 
gghistogram(low_gdp_cap, main = 'LLS: Low GDP per Capita Countries', x = 'Life.Ladder', fill= 'steelblue', bins = 10,  xlab = 'Life Ladder Scores', ylab = 'Count')
```

Density plots are used below to better assess the correlation between the samples and the normal distribution. Black dots closely positioned to the 45 degrees reference line would suggest a data distribution close to the normal distribution.

```{r, echo=FALSE, comment="", fig.width=4, fig.height=2, fig.align='center'}
# Observe the density plots of both variables versus the normal distribution 
ggqqplot(high_gdp_cap$Life.Ladder, title = "LLS High GDP per Capita Countries vs Normal Dist." )
```

```{r, echo=FALSE, comment="", fig.width=4, fig.height=2, fig.align='center'}
# Observe the density plots of both variables versus the normal distribution
ggqqplot(low_gdp_cap$Life.Ladder, title = "LLS Low GDP per Capita Countries vs Normal Dist.")
```

The black dots fluctuate around the reference line without forming a straight line. The visual study of this distribution does not provide a clear yes/no answer.
To quantitatively assess the distribution, a Shapiro-Wilk normality test has been performed on both Ladder scores random variables from Low and high GDP countries.
```{r, echo=FALSE, comment=""}
# Can do a Shapiro test as an extra source of evidence to test for normalcy
shapiro.test(low_gdp_cap$Life.Ladder)
```
p > 0.05 (barely) --> normal-ish

```{r, echo=FALSE, comment=""}
shapiro.test(low_gdp_cap$Log.GDP.per.capita)
#https://www.datanovia.com/en/lessons/normality-test-in-r/#check-normality-in-r 

```
p < 0.05 --> not normal 

```{r, echo=FALSE, comment=""}
# Can do a Shapiro test as an extra source of evidence to test for normalcy
shapiro.test(high_gdp_cap$Life.Ladder)
```
p < 0.05 --> not normal

```{r, echo=FALSE, comment=""}
shapiro.test(high_gdp_cap$Log.GDP.per.capita)
#https://www.datanovia.com/en/lessons/normality-test-in-r/#check-normality-in-r 

```
p < 0.05 --> not normal

The application of the test over the Life Ladder r.v from the Low GDP countries provides a p value barely superior to 0.05. The distribution may be normally distributed. 
Nevertheless, it is not the case for High GDP countries with p value inferior than 0.05. The second distribution can not be considered as normally distributed.
Hence, the third assumption is not verified.
\newline
**Conduct the test?**

\newline
Considering the fact that IID may not be satisfied and the random variables are not normally distributed, we conclude that the Two Sample t-Test is not applicable in this context.

### Legislators 

*Scenario:* We want to test whether Democratic or Republic senators are older, with two variables party and age (age needs to be calculated from DOB). 

*Proposed test:* Wilcoxon Rank Sum Test

*Test Assumptions:* \newline
1. Metric variable //Ordinal variables \newline
2. i.i.d. \newline
3. Same shape and spread of the the two variables \newline

```{r, echo=FALSE, comment=""}
# Load in Data set 
leg_data <- read.csv('datasets/legislators-current.csv')

# Select necessary variables and calculate age 
leg_data  <- leg_data %>% 
  select(birthday, party, type) %>%
  filter(type == 'sen') %>%
  mutate(age = as.numeric(difftime(Sys.Date(), as.Date(birthday), unit = "weeks"))/52.25)
leg_data <- leg_data %>% select(party, age)
```

```{r, echo=FALSE, comment=""}
# Split the data by party
dem_sen <- leg_data %>% filter(party == 'Democrat')
rep_sen <- leg_data %>% filter(party == 'Republican')
indep <- leg_data %>% filter(party == 'Independent') 
## We see all Senators have been accounted for as the number of rows of above 3 data frames adds up to 100 
```
Summary: Democrat Senators
```{r, echo=FALSE, comment=""}
# Get a quick look at the relevant variables (Democrats and Republicans only) 
summary(dem_sen$age)
```
Summary: Republican Senators
```{r, echo=FALSE, comment=""}
# Get a quick look at the relevant variables (Democrats and Republicans only) 
summary(rep_sen$age)
```

```{r, echo=FALSE, comment="", fig.width=4, fig.height=2, fig.align='center'}
# Check whether the variables have the similar shape/spread - box whisker plot or histogram 
gghistogram(dem_sen, main = 'Ages of Democrat Senators', x = 'age', fill= 'steelblue', bins = 5, xlab = 'Age', ylab = 'Count')
```

```{r, echo=FALSE, comment="", fig.width=4, fig.height=2, fig.align='center'}
boxplot(x=dem_sen$age, data = dem_sen,
        main="Ages of Democrat Senators",
        ylab="Age",
        col = 'steelblue')
```

```{r, echo=FALSE, comment="", fig.width=4, fig.height=2, fig.align='center'}
# Histogram of Republican senator ages 
gghistogram(rep_sen, main = 'Ages of Republican Senators', x = 'age', fill= 'steelblue', bins = 5, xlab = 'Age', ylab = 'Count')
# Check for some metric on similarities between the two variable distributions
```

```{r, echo=FALSE, comment="", fig.width=4, fig.height=2, fig.align='center'}
boxplot(x=rep_sen$age, data = rep_sen,
        main="Ages of Republican Senators",
        ylab="Age",
        col = 'steelblue')
```

**Conduct the test?**
\newline
The Wilcoxon Rank Sum Test is less restrictive than the Two Sample t-Test. The metric assumption and IID are still required but not the normality of the data anymore.
The legislators' age variable is made of continuous values. Data can be classified and the distances between values make sense. The variable is indeed a metric variable which satisfies the first assumption. 
Considering IID; All Republicans and Democrats' age are from the same distribution, namely politicians' ages. It satisfies the identically distributed consideration. Moreover, a politician's age doess not provide information about the age of any other politician, which satisfies the independence condition and thus the IID. Considering these elements, the Wilcoxon Rank Sum test is applicable.

### Wine and Health 

*Scenario:* We want to test whether these countries have more deaths from heart disease or liver disease.

*Proposed test:* Wilxocon Signed Rank Test

*Test Assumptions:* \newline
1. Metric Variables \newline
2. i.i.d.\newline
3. Paired data \newline
4. Difference is symmetric\newline
```{r, echo=FALSE, comment=""}
# Load in dataset 
library(wooldridge)
wine_data  <- wine

# Select necessary variables
wine_data <- wine_data %>% 
  select(country, heart, liver)
```

```{r, echo=FALSE, comment="", fig.width=4, fig.height=2, fig.align='center'}
# Test for symmetry
## First calculate the difference between Heart and Liver 
wine_data <- wine_data %>% mutate(difference = heart - liver)
boxplot(x=wine_data$difference, data = wine_data,
        main="Difference between Deaths from Heart Disease - Liver Disease",
        ylab="Number of Deaths",
        col = 'steelblue')

```

```{r, echo=FALSE, comment="", fig.width=4, fig.height=2, fig.align='center'}
gghistogram(wine_data, main = 'g1', x = 'difference', y = '..density..', fill= 'steelblue', bins = 5, add_density = TRUE)

```

**Conduct the test?**
\newline
The "heart" and "liver" variables are both composed of values obtained by counting, which make these discrete variables.
Additionally, the values of both observations can be compared and ranked as they share the same zero-base reference and counting granularity. Both rv are measured on a similar metric scale. The first assumption is verified.

The distribution of the difference between the paired samples  "heart" and "liver" is slightly left skewed. Moreover the tight and left heights of the density curve at a similar distance to the median are not equivalent. This distribution is not perfectly symmetric around some median. However, we should not expect the differences in distribution to be perfectly symmetric especially when the sample size is small. Thus, the assumption of symmetry is difficult to assess here but might be considered as verified.

The Wilcoxon Signed Rank Test can be applied in that context, with some cautions. 

\newline

*Sources*

First box whisker done with advice from https://www.youtube.com/watch?v=Y4-wAT4SNM4&ab_channel=Dr.ToddGrande 
go to around 6 min

Second chart done with advice from 
https://www.datanovia.com/en/lessons/wilcoxon-test-in-r/


### Attitudes Toward the Religion 

*Scenario:* We would like to know whether the U.S. population feels more positive towards Protestants or Catholics. 

*Proposed Test:* Paired t-Test

*Test Assumptions:* \newline
1. Metric Variables\newline
2. i.i.d.\newline
3. Paired\newline
4. Normality\newline
```{r, echo=FALSE, comment=""}
# Load in dataset 
rel_data <- read.csv('datasets/GSS_religion.csv')

# Select necessary variables 
rel_data <- rel_data %>% select(cathtemp, prottemp)
```

```{r, echo=FALSE, comment=""}
# Get a quick look at the relevant variables 
summary(rel_data)
```

```{r, echo=FALSE, comment="", fig.width=4, fig.height=2, fig.align='center'}
#Observe the histograms for both variables 
gghistogram(rel_data$cathtemp, fill= 'steelblue', bins = 5, xlab = 'Feeling Thermometer Rating', ylab = 'Count', main = 'Distribution of US Population Feelings Towards Catholics')
```

```{r, echo=FALSE, comment="", fig.width=4, fig.height=2, fig.align='center'}
gghistogram(rel_data$prottemp, fill= 'steelblue', bins = 5, xlab = 'Feeling Thermometer Rating', ylab = 'Count', main = 'Distribution of US Population Feelings Towards Protestants')
```

```{r, echo=FALSE, comment="", fig.width=4, fig.height=2, fig.align='center'}
diff_prottemp_cathtemp <- as.numeric(rel_data$prottemp - rel_data$cathtemp)
gghistogram(diff_prottemp_cathtemp, fill= 'steelblue', bins = 5, xlab = 'Feeling Thermometer Rating', ylab = 'Count', main = 'Distribution of the differences between US Population Feelings toward Catholics and Protestants')
```

```{r, echo=FALSE, comment=""}
# Conduct a Shapiro test to also help determine normalcy 
shapiro.test(rel_data$cathtemp)
```
p-value < 0.05 --> not normal 

```{r, echo=FALSE, comment=""}
# Conduct a Shapiro test to also help determine normalcy 
shapiro.test(rel_data$prottemp)
```
p-value < 0.05 --> not normal 

```{r, echo=FALSE, comment=""}
# Conduct a Shapiro test to also help determine normalcy 
shapiro.test(diff_prottemp_cathtemp)
```

**Conduct the test?** 
\newline
The distribution of the differences between US population feelings toward Catholics and Protestants is not normally distributed (considering both graph and Shapiro test values). The normality distribution of differences is one of the key criteria to apply a Paired t-test. The normality assumption is not verified, hence the paired t-test can not be applied in this context.

*Note on Shapiro wilks test*


The Shapiro-Wilk test is a statistical test used to check if a continuous variable follows a normal distribution. The null hypothesis (H0) states that the variable is normally distributed, and the alternative hypothesis (H1) states that the variable is NOT normally distributed. So after running this test:

If p <= 0.05: then the null hypothesis can be rejected (i.e. the variable is NOT normally distributed).
If p > 0.05: then the null hypothesis cannot be rejected (i.e. the variable MAY BE normally distributed).


*Source*

https://quantifyinghealth.com/report-shapiro-wilk-test/

